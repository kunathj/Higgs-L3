{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Higgs@L3_Logistic_Regression\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "The BDTs are used as a precut and must thus be already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import helpers\n",
    "from load_data import getTrainAndTest\n",
    "from plotting import confusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "As mentioned above, the BDT response is used for a precut on the data.\n",
    "We want to ignore the `mmis` variable.\n",
    "It is identified as a strong discriminator.\n",
    "It is nevertheless not used in the Classifier.\n",
    "Instead, it will later be used, together with the Classifier's response, in a 2D analysis.\n",
    "\n",
    "### Upweighting the signal\n",
    "\n",
    "The vast majority of events is background events. When using the weighted events, the best classifier will simply classify any event as background.\n",
    "We will upweight the signal such that the same total weight is given to the used background events as to the signal events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_train_w, X_test_w = getTrainAndTest(\"higgs_85\", bdt_precut=True, drop=[\"mmis\"], upweight_signal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Logistic Regression\n",
    "\n",
    "This is executed & evaluated for each of the mass hypothesis.\n",
    "The coefficients are saved in a plain `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_c = 0\n",
    "best_test_score = 0\n",
    "for c in [10**x for x in [-5, -2, 0, 2, 5, 7, 10]]:\n",
    "    lr = LogisticRegression(C=c).fit(X_train, y_train, sample_weight=X_train_w)\n",
    "    test_score = lr.score(X_test, y_test, sample_weight=X_test_w)\n",
    "    print(f\"Regularization with C={c:.0e}. Test set score = {100*test_score:.2f} %.\")\n",
    "    if test_score > best_test_score:\n",
    "        best_test_score = test_score\n",
    "        best_c = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_regs = {}\n",
    "for higgs_mass in helpers.higgs_hypotheses:\n",
    "    X_train, X_test, y_train, y_test, X_train_w, X_test_w = getTrainAndTest(higgs_mass, bdt_precut=True, drop=[\"mmis\"], upweight_signal=True)\n",
    "    log_regs[higgs_mass] = LogisticRegression(C=best_c).fit(X_train, y_train, sample_weight=X_train_w\n",
    "    )\n",
    "\n",
    "    y_pred = log_regs[higgs_mass].predict(X_test)\n",
    "    confusionMatrix(y_pred, y_test)\n",
    "    plt.title(f\"Confusion Matrix ($m_\\mathrm{{H}}$ = {higgs_mass[-2:]} GeV)\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"{higgs_mass} training score: {100*log_regs[higgs_mass].score(X_train, y_train, sample_weight=X_train_w):.2f} %.\")\n",
    "    print(f\"{higgs_mass} test set score: {100*log_regs[higgs_mass].score(X_test, y_test, sample_weight=X_test_w):.2f} %.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for higgs_mass, log_reg in log_regs.items():\n",
    "    plt.plot(log_reg.coef_.T, helpers.symbol[higgs_mass], \n",
    "             label=f\"$m_\\mathrm{{H}}$ = {higgs_mass[-2:]} GeV\")\n",
    "plt.title(\"Logistic Regression after BDT preselection\")\n",
    "plt.xlabel(\"feature $f_i$\")\n",
    "plt.ylabel(\"coefficient $\\\\alpha_i$\")\n",
    "plt.xticks(range(len(X_train.columns)), X_train.columns, rotation=90)\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/logisitic_regression_coefficients.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model coefficients and the intercepts for future usage\n",
    "\n",
    "Only save them if no other coefficients were saved yet, to avoid overwriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for higgs_mass, log_reg in log_regs.items():\n",
    "    coefs = np.concatenate([log_reg.coef_[0], log_reg.intercept_])\n",
    "    helpers.log_reg_coeffs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not (helpers.log_reg_coeffs_dir / f\"{higgs_mass}.txt\").exists():\n",
    "        np.savetxt(helpers.log_reg_coeffs_dir / f\"{higgs_mass}.txt\", coefs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('Higgs-L3': conda)",
   "language": "python",
   "name": "python38564bithiggsl3conda141e2cf07ec6480bb46052f1e5673410"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
